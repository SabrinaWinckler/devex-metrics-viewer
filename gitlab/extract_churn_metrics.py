#!/usr/bin/env python3
"""
Script para extrair m√©tricas de churn de commits e MRs do GitLab
Gera arquivos CSV com m√©tricas detalhadas por reposit√≥rio, autor e per√≠odo
"""

import pandas as pd
import os
import glob
import argparse
from datetime import datetime
import re

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Extrair m√©tricas de churn do GitLab',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemplos de uso:
  # Processar todos os arquivos normalized
  python extract_churn_metrics.py

  # Gerar sa√≠das individuais (uma linha por commit / MR)
  python extract_churn_metrics.py --individual

  # Especificar diret√≥rio de entrada e arquivos de sa√≠da
  python extract_churn_metrics.py --input-dir ./normalized \
      --commit-output ../consolidated/churn_results/commit_churn_gitlab.csv \
      --mr-output ../consolidated/churn_results/mr_churn_gitlab.csv --individual
        """
    )
    
    parser.add_argument('--input-dir', type=str, default='./normalized',
                        help='Diret√≥rio dos arquivos normalized (padr√£o: ./normalized)')
    parser.add_argument('--commit-output', type=str, default='commit_churn.csv',
                        help='Arquivo CSV de sa√≠da para commit churn (padr√£o: commit_churn.csv)')
    parser.add_argument('--mr-output', type=str, default='mr_churn.csv',
                        help='Arquivo CSV de sa√≠da para MR churn (padr√£o: mr_churn.csv)')
    parser.add_argument('--individual', action='store_true',
                        help='Gerar sa√≠das individuais (uma linha por commit / MR) em vez de agregar por autor/repositorio')
    
    return parser.parse_args()

def extract_period_from_filename(filename):
    """
    Extrair per√≠odo (YYYYMM) do nome do arquivo
    Exemplo: gitlab_commits_20250701_to_20250730_20251007_153111.csv -> 202507
    """
    # Padr√£o para extrair data inicial: YYYYMMDD
    match = re.search(r'_(\d{8})_to_', filename)
    if match:
        date_str = match.group(1)
        # Converter YYYYMMDD para YYYYMM
        return date_str[:6]
    
    return None

def get_repo_name(repo_path):
    """
    Extrair nome do reposit√≥rio do caminho completo
    Exemplo: grupo/subgrupo/repo-name -> repo-name
    """
    if pd.isna(repo_path) or not repo_path:
        return 'unknown'
    
    return repo_path.split('/')[-1]

def process_commit_churn(input_dir, individual=False):
    """
    Processar arquivos de commits para extrair m√©tricas de churn
    If individual=True, produce one output line per commit (no aggregation)
    """
    print("üìä Processando m√©tricas de churn de commits...")
    
    # Buscar todos os arquivos de commits
    commit_files = glob.glob(os.path.join(input_dir, 'gitlab_commits_*.csv'))
    
    all_commit_data = []
    
    for file_path in commit_files:
        print(f"   üìÑ Processando: {os.path.basename(file_path)}")
        
        # Extrair per√≠odo do nome do arquivo
        period = extract_period_from_filename(os.path.basename(file_path))
        if not period:
            print(f"   ‚ö†Ô∏è N√£o foi poss√≠vel extrair per√≠odo de: {file_path}")
            continue
        
        year = period[:4]
        month = period[4:]
        
        try:
            # Carregar arquivo CSV
            df = pd.read_csv(file_path)
            
            # Verificar se tem as colunas necess√°rias
            required_cols = ['repository', 'anonymized_name', 'lines_added', 'lines_deleted']
            if not all(col in df.columns for col in required_cols):
                print(f"   ‚ö†Ô∏è Colunas necess√°rias n√£o encontradas em: {file_path}")
                continue
            
            # Converter colunas num√©ricas
            df['lines_added'] = pd.to_numeric(df['lines_added'], errors='coerce').fillna(0)
            df['lines_deleted'] = pd.to_numeric(df['lines_deleted'], errors='coerce').fillna(0)
            
            # Filtrar registros v√°lidos
            df = df[df['anonymized_name'].notna() & (df['anonymized_name'] != 'P n/a')]
            df = df[df['repository'].notna()]
            
            if individual:
                # One row per commit
                for _, row in df.iterrows():
                    author = row['anonymized_name']
                    repo_path = row['repository']
                    repo_name = get_repo_name(repo_path)
                    repo_slug = repo_name
                    la = int(row['lines_added'])
                    ld = int(row['lines_deleted'])
                    total_churn = la + ld
                    net_change = la - ld
                    all_commit_data.append({
                        'period': period,
                        'year': year,
                        'month': month,
                        'repo_slug': repo_slug,
                        'repo_name': repo_name,
                        'author': author,
                        'commits': 1,
                        'lines_added': la,
                        'lines_removed': ld,
                        'total_churn': total_churn,
                        'net_change': net_change
                    })
            else:
                # Agrupar por reposit√≥rio e autor (comportamento antigo)
                grouped = df.groupby(['repository', 'anonymized_name']).agg({
                    'lines_added': 'sum',
                    'lines_deleted': 'sum',
                    'anonymized_name': 'count'  # Contar commits
                }).rename(columns={'anonymized_name': 'commits'})
                
                # Calcular m√©tricas de churn
                for (repo_path, author), stats in grouped.iterrows():
                    repo_name = get_repo_name(repo_path)
                    repo_slug = repo_name  # Usar nome como slug
                    
                    total_churn = stats['lines_added'] + stats['lines_deleted']
                    net_change = stats['lines_added'] - stats['lines_deleted']
                    
                    all_commit_data.append({
                        'period': period,
                        'year': year,
                        'month': month,
                        'repo_slug': repo_slug,
                        'repo_name': repo_name,
                        'author': author,
                        'commits': int(stats['commits']),
                        'lines_added': int(stats['lines_added']),
                        'lines_removed': int(stats['lines_deleted']),
                        'total_churn': int(total_churn),
                        'net_change': int(net_change)
                    })
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro ao processar {file_path}: {e}")
            continue
    
    print(f"   ‚úì {len(all_commit_data)} registros de commit churn processados")
    return all_commit_data

def process_mr_churn(input_dir, individual=False):
    """
    Processar arquivos de MRs para extrair m√©tricas de churn
    If individual=True, produce one output line per MR (no aggregation)
    """
    print("üîÄ Processando m√©tricas de churn de MRs...")
    
    # Buscar todos os arquivos de MRs
    mr_files = glob.glob(os.path.join(input_dir, 'gitlab_mrs_*.csv'))
    
    all_mr_data = []
    
    for file_path in mr_files:
        print(f"   üìÑ Processando: {os.path.basename(file_path)}")
        
        # Extrair per√≠odo do nome do arquivo
        period = extract_period_from_filename(os.path.basename(file_path))
        if not period:
            print(f"   ‚ö†Ô∏è N√£o foi poss√≠vel extrair per√≠odo de: {file_path}")
            continue
        
        year = period[:4]
        month = period[4:]
        
        try:
            # Carregar arquivo CSV
            df = pd.read_csv(file_path)
            
            # Verificar se tem as colunas necess√°rias
            required_cols = ['repository', 'anonymized_name', 'lines_added', 'lines_deleted']
            if not all(col in df.columns for col in required_cols):
                print(f"   ‚ö†Ô∏è Colunas necess√°rias n√£o encontradas em: {file_path}")
                continue
            
            # Converter colunas num√©ricas
            df['lines_added'] = pd.to_numeric(df['lines_added'], errors='coerce').fillna(0)
            df['lines_deleted'] = pd.to_numeric(df['lines_deleted'], errors='coerce').fillna(0)
            
            # Filtrar registros v√°lidos
            df = df[df['anonymized_name'].notna() & (df['anonymized_name'] != 'P n/a')]
            df = df[df['repository'].notna()]
            
            if individual:
                # One row per MR
                for _, row in df.iterrows():
                    author = row['anonymized_name']
                    repo_path = row['repository']
                    repo_name = get_repo_name(repo_path)
                    repo_slug = repo_name
                    la = int(row['lines_added'])
                    ld = int(row['lines_deleted'])
                    total_churn = la + ld
                    net_change = la - ld
                    avg_churn_per_pr = total_churn  # single PR
                    all_mr_data.append({
                        'period': period,
                        'year': year,
                        'month': month,
                        'repo_slug': repo_slug,
                        'repo_name': repo_name,
                        'author': author,
                        'prs': 1,
                        'lines_added': la,
                        'lines_removed': ld,
                        'total_churn': total_churn,
                        'net_change': net_change,
                        'avg_churn_per_pr': float(avg_churn_per_pr)
                    })
            else:
                # Agrupar por reposit√≥rio e autor (comportamento antigo)
                grouped = df.groupby(['repository', 'anonymized_name']).agg({
                    'lines_added': 'sum',
                    'lines_deleted': 'sum',
                    'anonymized_name': 'count'  # Contar MRs
                }).rename(columns={'anonymized_name': 'prs'})
                
                # Calcular m√©tricas de churn
                for (repo_path, author), stats in grouped.iterrows():
                    repo_name = get_repo_name(repo_path)
                    repo_slug = repo_name  # Usar nome como slug
                    
                    total_churn = stats['lines_added'] + stats['lines_deleted']
                    net_change = stats['lines_added'] - stats['lines_deleted']
                    avg_churn_per_pr = total_churn / stats['prs'] if stats['prs'] > 0 else 0
                    
                    all_mr_data.append({
                        'period': period,
                        'year': year,
                        'month': month,
                        'repo_slug': repo_slug,
                        'repo_name': repo_name,
                        'author': author,
                        'prs': int(stats['prs']),
                        'lines_added': int(stats['lines_added']),
                        'lines_removed': int(stats['lines_deleted']),
                        'total_churn': int(total_churn),
                        'net_change': int(net_change),
                        'avg_churn_per_pr': round(avg_churn_per_pr, 1)
                    })
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erro ao processar {file_path}: {e}")
            continue
    
    print(f"   ‚úì {len(all_mr_data)} registros de MR churn processados")
    return all_mr_data

def save_churn_data(data, output_file, data_type):
    """
    Salvar dados de churn em arquivo CSV
    """
    if not data:
        print(f"   ‚ö†Ô∏è Nenhum dado de {data_type} para salvar")
        return
    
    df = pd.DataFrame(data)
    
    # Ordenar por per√≠odo, reposit√≥rio e autor
    sort_cols = ['period', 'repo_name', 'author']
    existing_sort = [c for c in sort_cols if c in df.columns]
    if existing_sort:
        df = df.sort_values(existing_sort)
    
    # Salvar CSV
    df.to_csv(output_file, index=False)
    print(f"   ‚úì {len(df)} registros de {data_type} salvos em: {output_file}")

def main():
    args = parse_args()
    
    print("="*60)
    print("üìä EXTRATOR DE M√âTRICAS DE CHURN - GitLab")
    print("="*60)
    
    # Verificar se diret√≥rio de entrada existe
    if not os.path.exists(args.input_dir):
        print(f"‚ùå Diret√≥rio n√£o encontrado: {args.input_dir}")
        return
    
    print(f"\nüìÇ Diret√≥rio de entrada: {args.input_dir}")
    print(f"üìÑ Arquivo de sa√≠da (commits): {args.commit_output}")
    print(f"üìÑ Arquivo de sa√≠da (MRs): {args.mr_output}")
    print(f"üìÑ Modo individual (sem agrega√ß√£o): {args.individual}")
    
    # Processar m√©tricas de churn de commits
    commit_data = process_commit_churn(args.input_dir, individual=args.individual)
    save_churn_data(commit_data, args.commit_output, 'commit churn')
    
    print()
    
    # Processar m√©tricas de churn de MRs
    mr_data = process_mr_churn(args.input_dir, individual=args.individual)
    save_churn_data(mr_data, args.mr_output, 'MR churn')
    
    print("\n" + "="*60)
    print("‚úÖ EXTRA√á√ÉO CONCLU√çDA!")
    print("="*60)
    
    # Mostrar estat√≠sticas finais
    if commit_data:
        print(f"\nüìä Estat√≠sticas de Commit Churn:")
        commit_df = pd.DataFrame(commit_data)
        print(f"   ‚Ä¢ Total de registros: {len(commit_df)}")
        print(f"   ‚Ä¢ Per√≠odos √∫nicos: {commit_df['period'].nunique()}")
        print(f"   ‚Ä¢ Reposit√≥rios √∫nicos: {commit_df['repo_name'].nunique()}")
        print(f"   ‚Ä¢ Autores √∫nicos: {commit_df['author'].nunique()}")
        print(f"   ‚Ä¢ Total de commits: {commit_df['commits'].sum()}")
        print(f"   ‚Ä¢ Total churn: {commit_df['total_churn'].sum():,}")
    
    if mr_data:
        print(f"\nüîÄ Estat√≠sticas de MR Churn:")
        mr_df = pd.DataFrame(mr_data)
        print(f"   ‚Ä¢ Total de registros: {len(mr_df)}")
        print(f"   ‚Ä¢ Per√≠odos √∫nicos: {mr_df['period'].nunique()}")
        print(f"   ‚Ä¢ Reposit√≥rios √∫nicos: {mr_df['repo_name'].nunique()}")
        print(f"   ‚Ä¢ Autores √∫nicos: {mr_df['author'].nunique()}")
        print(f"   ‚Ä¢ Total de MRs: {mr_df['prs'].sum()}")
        print(f"   ‚Ä¢ Total churn: {mr_df['total_churn'].sum():,}")
    
    print()

if __name__ == '__main__':
    main()